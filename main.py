"""
å¤šæºå¿«è®¯èšåˆæ¨é€ å®æ—¶å¿«è®¯æ¨é€ï¼ˆakshareï¼‰
- ç»Ÿä¸€æ ¼å¼åŒ–è¾“å‡º
- é’‰é’‰æ¨é€
- è‡ªåŠ¨å»é‡ï¼ˆsent_ids + å†…å®¹ç›¸ä¼¼åº¦ï¼‰
- æœ¬åœ° JSON æŒä¹…åŒ–ï¼Œç¨‹åºé‡å¯ä»å¯é¿å…é‡å¤æ¨é€
- å®šæœŸæ¸…ç†æ—§è®°å½•
ä¾èµ–: pip install akshare requests pandas
"""
import logging
import time
from datetime import datetime, timezone, timedelta
import requests
import akshare as ak
import pandas as pd
import json
import os
from difflib import SequenceMatcher
import hashlib
from collections import deque
import re
from typing import Dict, List, Set, Any
from dataclasses import dataclass
from logger_util import setup_logger

logger = setup_logger("app_log", log_dir="logs", level=logging.INFO)

# ========== é…ç½® ==========
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=4bcc16f75f95ee7d0235902664f5bc8bf530285b4a73edc6224d90f15deea0a8"
FETCH_INTERVAL = 30      # ç§’
MAX_SENT_IDS = 10000     # æœ€å¤šä¿ç•™è®°å½•
MAX_AGE = 24 * 3600      # ä¿ç•™æ—¶é—´ï¼ˆç§’ï¼‰
CLEAN_INTERVAL = 1440    # æ¯å¤šå°‘è½®æ¸…ç†ä¸€æ¬¡(24*(3600/30))
SENT_IDS_FILE = "sent_ids.json"  # æœ¬åœ°æŒä¹…åŒ–æ–‡ä»¶
RECENT_CONTENTS_FILE = "recent_contents.json"  # æœ€è¿‘å†…å®¹æŒä¹…åŒ–æ–‡ä»¶

# é’‰é’‰æ¨é€é™æµ
MAX_PER_MINUTE = 20
# ==========================

@dataclass
class NewsItem:
    """æ–°é—»é¡¹æ•°æ®ç±»"""
    source: str
    timestamp: str
    content: str
    uid: str = ""

    def __post_init__(self):
        if not self.uid:
            self.uid = self.generate_uid()

    def generate_uid(self) -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        key = f"{self.source}_{self.timestamp}_{self.content}"
        return hashlib.md5(key.encode("utf-8")).hexdigest()

    def to_dict(self) -> Dict[str, Any]:
        return {
            "source": self.source,
            "timestamp": self.timestamp,
            "content": self.content,
            "uid": self.uid
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'NewsItem':
        return cls(
            source=data["source"],
            timestamp=data["timestamp"],
            content=data["content"],
            uid=data.get("uid", "")
        )


class NewsProcessor:
    """æ–°é—»å¤„ç†å™¨åŸºç±»"""

    def __init__(self, source_name: str):
        self.source_name = source_name

    def fetch_news(self) -> List[NewsItem]:
        """è·å–æ–°é—»æ•°æ®ï¼Œå­ç±»éœ€å®ç°"""
        raise NotImplementedError


class CLSProcessor(NewsProcessor):
    """è´¢è”ç¤¾å¤„ç†å™¨"""

    def __init__(self):
        super().__init__("è´¢è”ç¤¾")

    def fetch_news(self) -> List[NewsItem]:
        try:
            df = ak.stock_info_global_cls(symbol="é‡ç‚¹")
            if df is None or df.empty:
                return []

            news_items = []
            for _, row in df.iterrows():
                timestamp = f"{row['å‘å¸ƒæ—¥æœŸ']} {row['å‘å¸ƒæ—¶é—´']}"
                content = f"{row['æ ‡é¢˜']}ï¼š{row['å†…å®¹']}" if pd.notna(row['æ ‡é¢˜']) and str(row['æ ‡é¢˜']).strip() else str(row['å†…å®¹'])
                news_items.append(NewsItem(
                    source=self.source_name,
                    timestamp=timestamp,
                    content=content
                ))
            return news_items
        except Exception as e:
            logger.error(f"[x] è´¢è”ç¤¾æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return []


class SinaProcessor(NewsProcessor):
    """æ–°æµªè´¢ç»å¤„ç†å™¨"""

    def __init__(self):
        super().__init__("æ–°æµªè´¢ç»")

    def fetch_news(self) -> List[NewsItem]:
        try:
            df = ak.stock_info_global_sina()
            if df is None or df.empty:
                return []

            news_items = []
            for _, row in df.iterrows():
                timestamp = pd.to_datetime(row["æ—¶é—´"], errors="coerce").strftime("%Y-%m-%d %H:%M:%S")
                content = str(row["å†…å®¹"])
                news_items.append(NewsItem(
                    source=self.source_name,
                    timestamp=timestamp,
                    content=content
                ))
            return news_items
        except Exception as e:
            logger.error(f"[x] æ–°æµªè´¢ç»æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return []


class THSProcessor(NewsProcessor):
    """åŒèŠ±é¡ºå¤„ç†å™¨"""

    def __init__(self):
        super().__init__("åŒèŠ±é¡º")

    def fetch_news(self) -> List[NewsItem]:
        try:
            df = ak.stock_info_global_ths()
            if df is None or df.empty:
                return []

            news_items = []
            for _, row in df.iterrows():
                local_dt = datetime.strptime(row["å‘å¸ƒæ—¶é—´"], "%Y-%m-%d %H:%M:%S")
                timestamp = int(local_dt.timestamp())
                timestamp = datetime.fromtimestamp(timestamp, tz=timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S")
                content = f"{row['æ ‡é¢˜']}ï¼š{row['å†…å®¹']}" if pd.notna(row['æ ‡é¢˜']) and str(row['æ ‡é¢˜']).strip() else str(row['å†…å®¹'])
                news_items.append(NewsItem(
                    source=self.source_name,
                    timestamp=timestamp,
                    content=content
                ))
            return news_items
        except Exception as e:
            logger.error(f"[x] åŒèŠ±é¡ºæ•°æ®å¤„ç†å¤±è´¥: {e}")
            return []


class DingTalkSender:
    """é’‰é’‰æ¶ˆæ¯å‘é€å™¨"""

    def __init__(self, webhook: str, max_per_minute: int = 20):
        self.webhook = webhook
        self.max_per_minute = max_per_minute
        self.last_minute = time.time()
        self.sent_count = 0

    def send_message(self, news_item: NewsItem) -> bool:
        """å‘é€æ¶ˆæ¯åˆ°é’‰é’‰"""
        # é™æµæ§åˆ¶
        now = time.time()
        if now - self.last_minute >= 60:
            self.last_minute = now
            self.sent_count = 0

        if self.sent_count >= self.max_per_minute:
            sleep_time = 60 - (now - self.last_minute)
            logger.info(f"ğŸš¦ è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {sleep_time:.1f} ç§’")
            time.sleep(sleep_time)
            self.last_minute = time.time()
            self.sent_count = 0

        # å‘é€æ¶ˆæ¯
        msg = f"ğŸ“°ã€{news_item.source}ã€‘{news_item.timestamp}\n{news_item.content}"
        payload = {"msgtype": "text", "text": {"content": msg}}

        try:
            response = requests.post(self.webhook, json=payload, timeout=10)
            if response.status_code == 200:
                self.sent_count += 1
                return True
            else:
                logger.error(f"[x] é’‰é’‰æ¨é€å¤±è´¥: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            logger.error(f"[x] é’‰é’‰æ¨é€å¼‚å¸¸: {e}")
            return False


class ContentDeduplicator:
    """å†…å®¹å»é‡å™¨"""

    def __init__(self, recent_contents_file: str, max_recent_contents: int = 1000):
        self.recent_contents_file = recent_contents_file
        self.max_recent_contents = max_recent_contents
        self.recent_contents: deque = deque(maxlen=max_recent_contents)
        self.load_recent_contents()

    def normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r"<.*?>", "", text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r"[^\w\u4e00-\u9fff\s]", " ", text)
        text = re.sub(r"\s+", " ", text).strip()
        # è½¬æ¢ä¸ºå°å†™
        return text.lower()

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()

    def is_similar_content(self, content: str, threshold: float = 0.8) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸æœ€è¿‘å†…å®¹ç›¸ä¼¼"""
        normalized = self.normalize_text(content)

        for old_content in self.recent_contents:
            if self.calculate_similarity(normalized, old_content) > threshold:
                return True
        return False

    def add_content(self, content: str):
        """æ·»åŠ å†…å®¹åˆ°æœ€è¿‘å†…å®¹åˆ—è¡¨"""
        normalized = self.normalize_text(content)
        self.recent_contents.append(normalized)
        self.save_recent_contents()

    def load_recent_contents(self):
        """åŠ è½½æœ€è¿‘å†…å®¹"""
        if os.path.exists(self.recent_contents_file):
            try:
                with open(self.recent_contents_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.recent_contents = deque(data, maxlen=self.max_recent_contents)
                logger.info(f"[åŠ è½½] {len(self.recent_contents)} æ¡æœ€è¿‘å†…å®¹å·²åŠ è½½")
            except Exception as e:
                logger.error(f"[x] åŠ è½½æœ€è¿‘å†…å®¹æ–‡ä»¶å¤±è´¥: {e}")
                self.recent_contents = deque(maxlen=self.max_recent_contents)

    def save_recent_contents(self):
        """ä¿å­˜æœ€è¿‘å†…å®¹"""
        try:
            with open(self.recent_contents_file, "w", encoding="utf-8") as f:
                json.dump(list(self.recent_contents), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"[x] ä¿å­˜æœ€è¿‘å†…å®¹æ–‡ä»¶å¤±è´¥: {e}")


class NewsAggregator:
    """æ–°é—»èšåˆå™¨"""

    def __init__(self):
        self.sent_ids: Dict[str, float] = {}
        self.deduplicator = ContentDeduplicator(RECENT_CONTENTS_FILE)
        self.dingtalk_sender = DingTalkSender(DINGTALK_WEBHOOK, MAX_PER_MINUTE)

        # æ³¨å†Œæ•°æ®æºå¤„ç†å™¨
        self.processors = [
            CLSProcessor(),
            SinaProcessor(),
            THSProcessor()
        ]

        self.loop_count = 0
        self.load_sent_ids()

    def load_sent_ids(self):
        """åŠ è½½å·²å‘é€ID"""
        if os.path.exists(SENT_IDS_FILE):
            try:
                with open(SENT_IDS_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # è¿‡æ»¤è¿‡æœŸè®°å½•
                    now = time.time()
                    self.sent_ids = {uid: ts for uid, ts in data.items() if now - ts < MAX_AGE}
                logger.info(f"[åŠ è½½] {len(self.sent_ids)} æ¡å†å²è®°å½•å·²åŠ è½½")
            except Exception as e:
                logger.error(f"[x] åŠ è½½ sent_ids æ–‡ä»¶å¤±è´¥: {e}")
                self.sent_ids = {}
        else:
            self.sent_ids = {}

    def save_sent_ids(self):
        """ä¿å­˜å·²å‘é€ID"""
        try:
            with open(SENT_IDS_FILE, "w", encoding="utf-8") as f:
                json.dump(self.sent_ids, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"[x] ä¿å­˜ sent_ids æ–‡ä»¶å¤±è´¥: {e}")

    def is_duplicate_news(self, news_item: NewsItem) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦é‡å¤"""
        # æ£€æŸ¥UIDæ˜¯å¦å·²å­˜åœ¨
        if news_item.uid in self.sent_ids:
            return True

        # æ£€æŸ¥å†…å®¹æ˜¯å¦ç›¸ä¼¼
        if self.deduplicator.is_similar_content(news_item.content):
            logger.info(f"[å»é‡] ç›¸ä¼¼å†…å®¹ä¸æ¨é€: ({news_item.source}){news_item.content[:100]}...")
            return True

        return False

    def process_and_send_news(self):
        """å¤„ç†å¹¶å‘é€æ–°é—»"""
        all_news: List[NewsItem] = []

        # ä»æ‰€æœ‰æ•°æ®æºè·å–æ–°é—»
        for processor in self.processors:
            try:
                news_items = processor.fetch_news()
                all_news.extend(news_items)
                logger.info(f"[{processor.source_name}] è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            except Exception as e:
                logger.error(f"[x] {processor.source_name} è·å–å¤±è´¥: {e}")

        if not all_news:
            logger.info("æœ¬è½®æ— æ–°é—»æ•°æ®")
            return 0

        # å»é‡å¹¶æ’åº
        unique_news = []
        seen_uids: Set[str] = set()

        for news_item in all_news:
            if news_item.uid not in seen_uids:
                seen_uids.add(news_item.uid)
                unique_news.append(news_item)

        # æŒ‰æ—¶é—´æ’åº
        unique_news.sort(key=lambda x: x.timestamp)

        # å‘é€æ–°æ–°é—»
        sent_count = 0
        for news_item in unique_news:
            if not self.is_duplicate_news(news_item):
                if self.dingtalk_sender.send_message(news_item):
                    # è®°å½•å·²å‘é€
                    self.sent_ids[news_item.uid] = time.time()
                    self.deduplicator.add_content(news_item.content)
                    sent_count += 1
                    logger.info(f"[âœ“] æ¨é€: [{news_item.source}] {news_item.timestamp} - {news_item.content[:100]}...")

                time.sleep(1)  # é¿å…å‘é€è¿‡å¿«

        return sent_count

    def cleanup_old_records(self):
        """æ¸…ç†æ—§è®°å½•"""
        now = time.time()
        before = len(self.sent_ids)

        # æ¸…ç†è¿‡æœŸè®°å½•
        self.sent_ids = {
            uid: ts for uid, ts in self.sent_ids.items()
            if now - ts < MAX_AGE
        }

        # é™åˆ¶æœ€å¤§æ•°é‡
        if len(self.sent_ids) > MAX_SENT_IDS:
            self.sent_ids = dict(
                sorted(self.sent_ids.items(), key=lambda x: x[1], reverse=True)[:MAX_SENT_IDS]
            )

        after = len(self.sent_ids)
        if before != after:
            logger.info(f"[æ¸…ç†] sent_ids: {before} â†’ {after} (æ—¶é—´çª—å£ {MAX_AGE/3600:.1f}h, æœ€å¤§ {MAX_SENT_IDS})")

    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        logger.info("ğŸš€ å¯åŠ¨å¤šæºå¿«è®¯èšåˆæ¨é€ç³»ç»Ÿ...")

        while True:
            self.loop_count += 1

            try:
                sent_count = self.process_and_send_news()
                logger.info(f"âœ… ç¬¬ {self.loop_count} è½®å®Œæˆï¼Œæ¨é€ {sent_count} æ¡æ–°é—»")

                # ä¿å­˜çŠ¶æ€
                self.save_sent_ids()

                # å®šæœŸæ¸…ç†
                if self.loop_count % CLEAN_INTERVAL == 0:
                    self.cleanup_old_records()

            except Exception as e:
                logger.error(f"[x] ä¸»å¾ªç¯å¼‚å¸¸: {e}")

            time.sleep(FETCH_INTERVAL)

def main():
    """ä¸»å‡½æ•°"""
    aggregator = NewsAggregator()
    aggregator.run()


if __name__ == "__main__":
    main()