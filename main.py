"""
å¤šæºå¿«è®¯èšåˆæ¨é€ å®æ—¶å¿«è®¯æ¨é€ï¼ˆakshareï¼‰
- ç»Ÿä¸€æ ¼å¼åŒ–è¾“å‡º
- é’‰é’‰æ¨é€
- è‡ªåŠ¨å»é‡ï¼ˆsent_ids + å†…å®¹ç›¸ä¼¼åº¦ï¼‰
- æœ¬åœ° JSON æŒä¹…åŒ–ï¼Œç¨‹åºé‡å¯ä»å¯é¿å…é‡å¤æ¨é€
- å®šæœŸæ¸…ç†æ—§è®°å½•
ä¾èµ–: pip install akshare requests pandas
"""
import logging
import time
from datetime import datetime, timezone, timedelta
import stock_info as ak
import pandas as pd
import json
import os
from difflib import SequenceMatcher
import hashlib
from collections import deque
import re
from typing import Dict, List, Set, Any
from dataclasses import dataclass

from dingTalk import DingTalkDispatcher
from logger_util import setup_logger

logger = setup_logger("app_log", log_dir="logs", level=logging.INFO)

# ========== é…ç½® ==========
FETCH_INTERVAL = 30      # ç§’
MAX_SENT_IDS = 10000     # æœ€å¤šä¿ç•™è®°å½•
MAX_AGE = 24 * 3600      # ä¿ç•™æ—¶é—´ï¼ˆç§’ï¼‰
CLEAN_INTERVAL = 1440    # æ¯å¤šå°‘è½®æ¸…ç†ä¸€æ¬¡(24*(3600/30))
STATE = "data/state.json"  # æœ€è¿‘å†…å®¹æŒä¹…åŒ–æ–‡ä»¶
DATA_FILE = "data/messages.json"

# ==========================

@dataclass
class NewsItem:
    """æ–°é—»é¡¹æ•°æ®ç±»"""
    source: str
    timestamp: str
    content: str
    uid: str = ""

    def __post_init__(self):
        if not self.uid:
            self.uid = self.generate_uid()

    def generate_uid(self) -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        key = f"{self.source}_{self.timestamp}_{self.content}"
        return hashlib.md5(key.encode("utf-8")).hexdigest()

    def to_dict(self) -> Dict[str, Any]:
        return {
            "source": self.source,
            "timestamp": self.timestamp,
            "content": self.content,
            "uid": self.uid
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'NewsItem':
        return cls(
            source=data["source"],
            timestamp=data["timestamp"],
            content=data["content"],
            uid=data.get("uid", "")
        )


class NewsProcessor:
    """æ–°é—»å¤„ç†å™¨åŸºç±»"""

    def __init__(self, source_name: str):
        self.source_name = source_name

    def fetch_news(self) -> List[NewsItem]:
        """è·å–æ–°é—»æ•°æ®ï¼Œå­ç±»éœ€å®ç°"""
        raise NotImplementedError


class CLSProcessor(NewsProcessor):
    """è´¢è”ç¤¾å¤„ç†å™¨"""

    def __init__(self):
        super().__init__("è´¢è”ç¤¾")

    def fetch_news(self) -> List[NewsItem]:
        try:
            logger.info("[è´¢è”ç¤¾] -> è·å–ä¿¡æ¯å¼€å§‹")
            # df = ak.stock_info_global_cls(symbol="å…¨éƒ¨")
            df = ak.stock_info_global_cls(symbol="é‡ç‚¹")
            if df is None or df.empty:
                return []

            news_items = []
            for _, row in df.iterrows():
                timestamp = pd.to_datetime(f"{row['å‘å¸ƒæ—¥æœŸ']} {row['å‘å¸ƒæ—¶é—´']}").strftime("%Y-%m-%d %H:%M:%S")
                content = f"{row['æ ‡é¢˜']}ï¼š{row['å†…å®¹']}" if pd.notna(row['æ ‡é¢˜']) and str(row['æ ‡é¢˜']).strip() else str(row['å†…å®¹'])
                news_items.append(NewsItem(
                    source=self.source_name,
                    timestamp=timestamp,
                    content=content
                ))
            return news_items
        except Exception as e:
            logger.error(f"[x] è´¢è”ç¤¾æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return []


class SinaProcessor(NewsProcessor):
    """æ–°æµªè´¢ç»å¤„ç†å™¨"""

    def __init__(self):
        super().__init__("æ–°æµªè´¢ç»")

    def fetch_news(self) -> List[NewsItem]:
        try:
            logger.info("[æ–°æµªè´¢ç»] -> è·å–ä¿¡æ¯å¼€å§‹")
            df = ak.stock_info_global_sina()
            if df is None or df.empty:
                return []

            news_items = []
            for _, row in df.iterrows():
                timestamp = pd.to_datetime(row["æ—¶é—´"]).strftime("%Y-%m-%d %H:%M:%S")
                content = str(row["å†…å®¹"])
                news_items.append(NewsItem(
                    source=self.source_name,
                    timestamp=timestamp,
                    content=content
                ))
            return news_items
        except Exception as e:
            logger.error(f"[x] æ–°æµªè´¢ç»æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return []


class THSProcessor(NewsProcessor):
    """åŒèŠ±é¡ºå¤„ç†å™¨"""

    def __init__(self):
        super().__init__("åŒèŠ±é¡º")

    def fetch_news(self) -> List[NewsItem]:
        try:
            logger.info("[åŒèŠ±é¡º] -> è·å–ä¿¡æ¯å¼€å§‹")
            df = ak.stock_info_global_ths()
            if df is None or df.empty:
                return []

            news_items = []
            for _, row in df.iterrows():
                local_dt = datetime.strptime(row["å‘å¸ƒæ—¶é—´"], "%Y-%m-%d %H:%M:%S")
                timestamp = int(local_dt.timestamp())
                timestamp = datetime.fromtimestamp(timestamp, tz=timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S")
                content = f"{row['æ ‡é¢˜']}ï¼š{row['å†…å®¹']}" if pd.notna(row['æ ‡é¢˜']) and str(row['æ ‡é¢˜']).strip() else str(row['å†…å®¹'])
                news_items.append(NewsItem(
                    source=self.source_name,
                    timestamp=timestamp,
                    content=content
                ))
            return news_items
        except Exception as e:
            logger.error(f"[x] åŒèŠ±é¡ºæ•°æ®å¤„ç†å¤±è´¥: {e}")
            return []

class StateManager:
    """ç»Ÿä¸€çŠ¶æ€ç®¡ç†å™¨ï¼Œåˆå¹¶ sent_ids + recent_contents"""

    def __init__(self, state_file: str, max_sent_ids: int = 10000, max_recent: int = 1000):
        self.state_file = state_file
        self.max_sent_ids = max_sent_ids
        self.max_recent = max_recent
        self.sent_ids: Dict[str, float] = {}
        self.recent_contents: deque = deque(maxlen=max_recent)
        self.load()

    def load(self):
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.sent_ids = data.get("sent_ids", {})
                    recent = data.get("recent_contents", [])
                    self.recent_contents = deque(recent, maxlen=self.max_recent)
                    logger.info(f"[åŠ è½½] çŠ¶æ€æ–‡ä»¶æˆåŠŸ: sent={len(self.sent_ids)}, recent={len(self.recent_contents)}")
            except Exception as e:
                logger.error(f"[x] åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def save(self):
        os.makedirs(os.path.dirname(self.state_file), exist_ok=True)
        try:
            with open(self.state_file, "w", encoding="utf-8") as f:
                json.dump({
                    "sent_ids": self.sent_ids,
                    "recent_contents": list(self.recent_contents)
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"[x] ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def add_sent(self, uid: str):
        self.sent_ids[uid] = time.time()
        # é™åˆ¶æ•°é‡
        if len(self.sent_ids) > self.max_sent_ids:
            # æŒ‰æ—¶é—´ä¿ç•™æœ€æ–°
            self.sent_ids = dict(sorted(self.sent_ids.items(), key=lambda x: x[1], reverse=True)[:self.max_sent_ids])

    def clean_old(self, max_age: int):
        now = time.time()
        before = len(self.sent_ids)
        self.sent_ids = {uid: ts for uid, ts in self.sent_ids.items() if now - ts < max_age}
        after = len(self.sent_ids)
        if before != after:
            logger.info(f"[æ¸…ç†] sent_ids: {before} â†’ {after}")

    def normalize_text(self, text: str) -> str:
        text = re.sub(r"<.*?>", "", text)
        text = re.sub(r"[^\w\u4e00-\u9fff\s]", " ", text)
        text = re.sub(r"\s+", " ", text).strip().lower()
        return text

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()

    def is_similar(self, text: str, threshold=0.8) -> bool:
        normalized = self.normalize_text(text)
        for old in self.recent_contents:
            if self.calculate_similarity(normalized, old) > threshold:
                return True
        return False

    def add_recent(self, text: str):
        normalized = self.normalize_text(text)
        self.recent_contents.append(normalized)

class NewsAggregator:
    """æ–°é—»èšåˆå™¨"""

    def __init__(self):
        self.state  = StateManager(STATE)
        self.dingtalk_sender = DingTalkDispatcher()
        # æ³¨å†Œæ•°æ®æºå¤„ç†å™¨
        self.processors = [
            CLSProcessor(),
            SinaProcessor(),
            THSProcessor()
        ]
        self.loop_count = 0

    def save_message_to_local(self, news: NewsItem):
        os.makedirs("data", exist_ok=True)
        messages = []

        if os.path.exists(DATA_FILE):
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                try:
                    messages = json.load(f)
                except json.JSONDecodeError:
                    messages = []

        new_item = {
            "id": news.uid,
            "time": news.timestamp,
            "source": news.source,
            "content": news.content
        }
        # æ’å…¥åˆ°æœ€å‰é¢
        messages.insert(0, new_item)
        # åªä¿ç•™æœ€æ–° 1000 æ¡
        messages = messages[:1000]

        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(messages, f, ensure_ascii=False, indent=2)

    def safe_parse_time(self, ts_str: str) -> float:
        """å°†æ—¶é—´å­—ç¬¦ä¸²å®‰å…¨è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆç»Ÿä¸€ä¸ºåŒ—äº¬æ—¶é—´ï¼‰"""
        try:
            dt = datetime.strptime(ts_str, "%Y-%m-%d %H:%M:%S")
            # ç¡®ä¿æŒ‰åŒ—äº¬æ—¶é—´ (+8)
            return dt.replace(tzinfo=timezone(timedelta(hours=8))).timestamp()
        except Exception:
            return 0.0

    def process_and_send_news(self):
        """å¤„ç†å¹¶å‘é€æ–°é—»"""
        all_news: List[NewsItem] = []

        # ä»æ‰€æœ‰æ•°æ®æºè·å–æ–°é—»
        for processor in self.processors:
            try:
                news_items = processor.fetch_news()
                all_news.extend(news_items)
                logger.info(f"[{processor.source_name}] è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            except Exception as e:
                logger.error(f"[x] {processor.source_name} è·å–å¤±è´¥: {e}")

        if not all_news:
            logger.info("æœ¬è½®æ— æ–°é—»æ•°æ®")
            return 0

        # å»é‡å¹¶æ’åº
        unique_news = []
        seen: Set[str] = set()

        for news_item in all_news:
            if news_item.uid not in seen:
                seen.add(news_item.uid)
                unique_news.append(news_item)

        # æŒ‰æ—¶é—´æ’åº
        unique_news.sort(key=lambda x: self.safe_parse_time(x.timestamp))

        # å‘é€æ–°æ–°é—»
        sent_count = 0
        for news_item in unique_news:
            if news_item.uid in self.state.sent_ids:
                continue
            if self.state.is_similar(news_item.content):
                logger.info(f"[å»é‡] ç›¸ä¼¼å†…å®¹è·³è¿‡: {news_item.content[:80]}...")
                continue

            self.save_message_to_local(news_item)
            self.state.add_sent(news_item.uid)
            self.state.add_recent(news_item.content)
            self.dingtalk_sender.enqueue_message(news_item)
            sent_count += 1
            logger.info(f"[âœ“] æ¨é€: [{news_item.source}] {news_item.timestamp} - {news_item.content[:100]}...")

            time.sleep(1)  # é¿å…å‘é€è¿‡å¿«

        # ä¿å­˜çŠ¶æ€
        self.state.save()
        return sent_count

    def cleanup_old_records(self):
        """æ¸…ç†æ—§è®°å½•"""
        self.state.clean_old(MAX_AGE)
        self.state.save()

    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        logger.info("ğŸš€ å¯åŠ¨å¤šæºå¿«è®¯èšåˆæ¨é€ç³»ç»Ÿ...")

        while True:
            self.loop_count += 1

            try:
                sent_count = self.process_and_send_news()
                logger.info(f"âœ… ç¬¬ {self.loop_count} è½®å®Œæˆï¼Œæ¨é€ {sent_count} æ¡æ–°é—»")

                # å®šæœŸæ¸…ç†
                if self.loop_count % CLEAN_INTERVAL == 0:
                    self.cleanup_old_records()

            except Exception as e:
                logger.error(f"[x] ä¸»å¾ªç¯å¼‚å¸¸: {e}")

            time.sleep(FETCH_INTERVAL)

def main():
    """ä¸»å‡½æ•°"""
    aggregator = NewsAggregator()
    aggregator.run()


if __name__ == "__main__":
    main()