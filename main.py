"""
å¤šæºå¿«è®¯èšåˆæ¨é€ å®æ—¶å¿«è®¯æ¨é€ï¼ˆakshareï¼‰
- ç»Ÿä¸€æ ¼å¼åŒ–è¾“å‡º
- é’‰é’‰æ¨é€
- è‡ªåŠ¨å»é‡ï¼ˆsent_idsï¼‰
- æœ¬åœ° JSON æŒä¹…åŒ–ï¼Œç¨‹åºé‡å¯ä»å¯é¿å…é‡å¤æ¨é€
- å®šæœŸæ¸…ç†æ—§è®°å½•
ä¾èµ–: pip install akshare requests pandas
"""
import logging
import time
import requests
import akshare as ak
import pandas as pd
import json
import os
from difflib import SequenceMatcher
import hashlib
from collections import deque
import re
from logger_util import setup_logger

logger = setup_logger("app_log", log_dir="logs", level=logging.INFO)

# ========== é…ç½® ==========
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token="
FETCH_INTERVAL = 30      # ç§’
MAX_SENT_IDS = 10000     # æœ€å¤šä¿ç•™è®°å½•
MAX_AGE = 24 * 3600       # ä¿ç•™æ—¶é—´ï¼ˆç§’ï¼‰
CLEAN_INTERVAL = 1440     # æ¯å¤šå°‘è½®æ¸…ç†ä¸€æ¬¡(24*(3600/30))
SENT_IDS_FILE = "sent_ids.json"  # æœ¬åœ°æŒä¹…åŒ–æ–‡ä»¶

# é’‰é’‰
MAX_PER_MINUTE = 20
last_minute = time.time()
sent_count = 0
# ==========================

# sent_ids: {uid: timestamp}
sent_ids = {}
loop_count = 0

# ä¿å­˜æœ€è¿‘å·²æ¨é€çš„å†…å®¹æ–‡æœ¬ï¼ˆé™å®šå®¹é‡ï¼Œé˜²å†…å­˜çˆ†ï¼‰
recent_contents = deque(maxlen=1000)

# ---------- åŠ è½½æœ¬åœ° sent_ids ----------
def load_sent_ids():
    global sent_ids
    if os.path.exists(SENT_IDS_FILE):
        try:
            with open(SENT_IDS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                # è¿‡æ»¤è¿‡æœŸè®°å½•
                now = time.time()
                sent_ids = {uid: ts for uid, ts in data.items() if now - ts < MAX_AGE}
                logger.info(f"[åŠ è½½] {len(sent_ids)} æ¡å†å²è®°å½•å·²åŠ è½½")
        except Exception as e:
            logger.error(f"[x] åŠ è½½ sent_ids æ–‡ä»¶å¤±è´¥: {e}")
            sent_ids = {}
    else:
        sent_ids = {}

# ---------- ä¿å­˜æœ¬åœ° sent_ids ----------
def save_sent_ids():
    try:
        with open(SENT_IDS_FILE, "w", encoding="utf-8") as f:
            json.dump(sent_ids, f, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[x] ä¿å­˜ sent_ids æ–‡ä»¶å¤±è´¥: {e}")


# ---------- é’‰é’‰æ¨é€ ----------
def send_to_dingtalk(source: str, ts: str, content: str):
    msg = f"ğŸ“°ã€{source}ã€‘{ts}\n{content}"
    payload = {"msgtype": "text", "text": {"content": msg}}
    try:
        requests.post(DINGTALK_WEBHOOK, json=payload, timeout=5)
    except Exception as e:
        logger.error(f"[x] é’‰é’‰æ¨é€å¤±è´¥: {e}")

def send_rate_limited(source, ts, content):
    global last_minute, sent_count
    now = time.time()
    if now - last_minute >= 60:
        last_minute = now
        sent_count = 0
    if sent_count >= MAX_PER_MINUTE:
        # ç­‰å¾…åˆ°ä¸‹ä¸€åˆ†é’Ÿ
        sleep_time = 60 - (now - last_minute)
        time.sleep(sleep_time)
        last_minute = time.time()
        sent_count = 0
    send_to_dingtalk(source, ts, content)
    sent_count += 1


# ---------- è´¢è”ç¤¾ ----------
def process_cls(symbol: str = "å…¨éƒ¨") -> pd.DataFrame:
    try:
        df = ak.stock_info_global_cls(symbol=symbol)
        if df is None or df.empty:
            return pd.DataFrame(columns=["æ—¶é—´", "å†…å®¹", "æ¥æº"])
        df["æ—¶é—´"] = df["å‘å¸ƒæ—¥æœŸ"].astype(str) + " " + df["å‘å¸ƒæ—¶é—´"].astype(str)
        df["å†…å®¹"] = df["æ ‡é¢˜"].astype(str) + "ï¼š" + df["å†…å®¹"].astype(str)
        df["æ¥æº"] = "è´¢è”ç¤¾"
        return df[["æ—¶é—´", "å†…å®¹", "æ¥æº"]]
    except Exception as e:
        logger.error(f"[x] è´¢è”ç¤¾æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return pd.DataFrame(columns=["æ—¶é—´", "å†…å®¹", "æ¥æº"])


# ---------- æ–°æµªè´¢ç» ----------
def process_sina() -> pd.DataFrame:
    try:
        df = ak.stock_info_global_sina()
        if df is None or df.empty:
            return pd.DataFrame(columns=["æ—¶é—´", "å†…å®¹", "æ¥æº"])

        df["æ—¶é—´"] = pd.to_datetime(df["æ—¶é—´"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
        df["å†…å®¹"] = df["å†…å®¹"].astype(str)
        df["æ¥æº"] = "æ–°æµªè´¢ç»"
        return df[["æ—¶é—´", "å†…å®¹", "æ¥æº"]]
    except Exception as e:
        logger.error(f"[x] æ–°æµªè´¢ç»æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return pd.DataFrame(columns=["æ—¶é—´", "å†…å®¹", "æ¥æº"])

# ---------- åŒèŠ±é¡ºè´¢ç» ----------
def process_ths() -> pd.DataFrame:
    try:
        df = ak.stock_info_global_ths()
        if df is None or df.empty:
            return pd.DataFrame(columns=["æ—¶é—´", "å†…å®¹", "æ¥æº"])

        df["æ—¶é—´"] = pd.to_datetime(df["å‘å¸ƒæ—¶é—´"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
        df["å†…å®¹"] = df["æ ‡é¢˜"].astype(str) + "ï¼š" + df["å†…å®¹"].astype(str)
        df["æ¥æº"] = "åŒèŠ±é¡º"
        return df[["æ—¶é—´", "å†…å®¹", "æ¥æº"]]
    except Exception as e:
        logger.error(f"[x] åŒèŠ±é¡ºç»æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return pd.DataFrame(columns=["æ—¶é—´", "å†…å®¹", "æ¥æº"])


def normalize_text(text: str) -> str:
    # å»é™¤HTMLæ ‡ç­¾ã€æ¢è¡Œç¬¦ã€å¤šä½™ç©ºæ ¼
    text = re.sub(r"<.*?>", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ------- è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ -------
def is_similar(a: str, b: str, threshold=0.85) -> bool:
    return SequenceMatcher(None, a, b).ratio() > threshold

def is_recently_sent(content: str) -> bool:
    """æ£€æŸ¥è¯¥å†…å®¹æ˜¯å¦ä¸æœ€è¿‘å·²æ¨é€å†…å®¹ç›¸ä¼¼"""
    normalized = normalize_text(content)
    for old in recent_contents:
        if is_similar(normalized, old):
            return True
    return False

# ---------- å”¯ä¸€ID ----------
def make_uid(row) -> str:
    key = f"{row.get('æ¥æº','')}_{row.get('æ—¶é—´','')}_{row.get('å†…å®¹','')}"
    return hashlib.md5(key.encode("utf-8")).hexdigest()


# ---------- ä¸»å¾ªç¯ ----------
def main_loop():
    global loop_count, sent_ids
    load_sent_ids()
    logger.info("ğŸš€ å¯åŠ¨ è´¢è”ç¤¾ + æ–°æµªè´¢ç» å®æ—¶å¿«è®¯æ¨é€ï¼ˆå«æŒä¹…åŒ–ï¼‰...\n")

    while True:
        loop_count += 1
        frames = []

        # --------------------------
        df_cls = process_cls()
        if not df_cls.empty:
            frames.append(df_cls)

        df_sina = process_sina()
        if not df_sina.empty:
            frames.append(df_sina)

        df_ths = process_ths()
        if not df_ths.empty:
            frames.append(df_ths)

        #---------------------------
        if not frames:
            logger.info("æ— æ•°æ®")
            time.sleep(FETCH_INTERVAL)
            continue

        df_all = pd.concat(frames, ignore_index=True)
        df_all = df_all.drop_duplicates(subset=["æ¥æº", "æ—¶é—´", "å†…å®¹"])
        df_all = df_all.sort_values("æ—¶é—´", ascending=True).reset_index(drop=True)

        new_count = 0
        for _, row in df_all.iterrows():
            uid = make_uid(row)
            if uid in sent_ids:
                continue
            # å»é‡ï¼šæ˜¯å¦ç›¸ä¼¼å†…å®¹å·²æ¨é€
            if is_recently_sent(row["å†…å®¹"]):
                logger.info(f"[{row['æ¥æº']}]ç›¸ä¼¼å†…å®¹ä¸æ¨é€ï¼")
                continue
            sent_ids[uid] = time.time()
            send_rate_limited(row["æ¥æº"], row["æ—¶é—´"], row["å†…å®¹"])
            logger.info(f"[âœ“] æ¨é€ï¼š[{row['æ¥æº']}] {row['æ—¶é—´']} - {row['å†…å®¹'][:120]}...")
            recent_contents.append(normalize_text(row["å†…å®¹"]))

            new_count += 1
            time.sleep(1)

        logger.info(f"âœ… æœ¬è½®æ¨é€ {new_count} æ¡")
        save_sent_ids()  # ä¿å­˜åˆ°æœ¬åœ° JSON

        # ---------- æ¸…ç† sent_ids ----------
        if loop_count % CLEAN_INTERVAL == 0:
            now = time.time()
            before = len(sent_ids)
            sent_ids = {
                uid: ts for uid, ts in sent_ids.items()
                if now - ts < MAX_AGE
            }
            if len(sent_ids) > MAX_SENT_IDS:
                sent_ids = dict(
                    sorted(sent_ids.items(), key=lambda x: x[1], reverse=True)[:MAX_SENT_IDS]
                )
            after = len(sent_ids)
            logger.info(f"[æ¸…ç†] sent_ids: {before} â†’ {after} (æ—¶é—´çª—å£ {MAX_AGE/3600:.1f}h, æœ€å¤§ {MAX_SENT_IDS})")

        time.sleep(FETCH_INTERVAL)


if __name__ == "__main__":
    main_loop()
